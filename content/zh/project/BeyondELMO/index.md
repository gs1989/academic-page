---
slides: Beyond ELMO
url_pdf: ""
title: ELMO的维护与更新
summary: ""
url_video: ""
date: 2021-04-19T18:41:50.241Z
categories: []
url_slides: ""
subtitle: 个人观点
tags:
  - ELMO
links:
  - icon: github
    icon_pack: fab
    name: ELMO Github
    url: https://github.com/sca-research/ELMO
  - url: http://reassure.eu/
    name: REASSURE项目网站
    icon_pack: fab
image:
  caption: ELMO工作流程
  focal_point: Smart
  filename: featured.png
url_code: ""
---
通常来说，泄漏分析工作异常繁琐: 用户需要将代码布置到目标设备上，然后搭建能够可靠地测量功耗的实验平台。考虑到很多密码工程师并不一定有扎实的电子背景，也未必接触过数字示波器，搭建平台本身就很容易成为挑战。另一方面，平台的搭建必须建立在代码已部署到目标设备上：从开发周期来看，进行泄漏分析和测试的步骤出现得过晚，也很容易使得大量的前期开发工作付诸东流。

显而易见，若有工具能在开发早期，无实物测量的条件下提供泄漏评估，对于开发者而言能大幅提高开发效率。举例来说，若开发目标运行于ARM处理器上，此类工具可以在完成代码开发之后立即进行泄漏测试，而无需等待实物进行部署测量。思路上，此类工具并不罕见：学术和业界都有相关的项目产出，如Riscure公司在Inspector套件中即有内置相关的功能。

根据具体的使用条件，相关工具可以分为算法级（由抽象过程或由C语言定义）或者电路级（如SPICE）。前者运行速度快，但缺乏大量的底层细节，容易错失泄漏点（例如汇编指令中产生的泄漏在C语言中不可见; 后者较为准确，但要求全电路公开（除厂商外难以获取），且速度慢。

ELMO的开发实际上可以看作以上两者的折中：ELMO读入二进制程序代码，并利用对实测数据profiling的统计信息补足处理器(微)架构细节。下图给出了ELMO的工作流程。Github的项目目录(https://github.com/sca-research/ELMO)给出了ELMO的技术细节(包括论文)。由于我本人未参与这部分工作，更多细节不再详细论述。以下仅论述我接手后我们考虑的部分改进。

ELMO中一个直接问题在于其模拟“质量”, 即其结果对比实测情况的相似度：然而，从上述表达中就不难看出，这种“质量”实际并没有严格的定义。更确切地说，不同研究者对于“质量”一词应该代表什么都没有共识; 这也是我在Bristol/Klagenfurt的研究面临的核心挑战。几年的研究中，我们做过以下尝试：

1) 换用另一款ARM M0处理器: ELMO理论上可用于任何基于16比特THUMB指令的代码，但其profiling建模只针对特定一款M0处理器(STM32F0)。若扩展到其他M0处理器上，虽然程序可以继续运作，其模型的通用性却需要重新评估。依靠Dr. Dan Page的SCALE项目(https://github.com/danpage/scale), 我们对另一款M0处理器, NXP LPC1114，搭建了稳定的测量环境。利用相同的建模方式，我们在这一平台上重建了ELMO模型并配置了相关参数。相关细节最早于2018 ARM Research峰会中披露(slides: https://github.com/sca-research/ELMO/blob/master/Modeling_M0_leakage_generically.pdf)。

2) 模拟准确性: 如果我们简单将“质量”定义成模拟准确性, 则在机器学习中常见的衡量标准交互验证。对于新M0平台我们进行了交互验证(结果见https://github.com/sca-research/ELMO/blob/master/ELMOCrossValidation.xlsx): 简而言之, 若建模和测试在统一的平台上，模拟准确性可保障；若非统一平台，则模型准确性欠佳。 

3) 其他度量: 考虑到具体的应用目标，用户不一定关注模拟准确性：例如，若最终泄漏分析的结果是TVLA检测，则目标为找出泄漏的时间和原因，而非准确地量化泄漏(T检测的性质只能做定性度量，无法定量)。此时，我们仅关注工具能否捕获所有实测中出现的泄漏以及归因，而非对功耗本身的准确模拟。然而，对于这种情况目前依然没有明确的度量方法：这部分也是我们的在研工作之一。

4) 基准度量: 以上的度量理论性较强，实践中解释不易。直截了当的评价方式是找出大量的掩码代码，并找出大量的已知缺陷，用其测评工具的效果。尽管该方法听起来简单直接，事实是开源范畴内的软件掩码代码并不多，而做过详细的实现缺陷分析的凤毛麟角。很多代码目前仍为算法级(即用C语言实现)，因而所有指令级，处理器架构级的缺陷被看作实现细节而忽略掉。由于ARM架构的底层细节不公开，而TVLA技术只报告泄漏位置，不可归因，详尽地记录所有泄漏点和成因需要消耗大量的时间和精力，而目前的学术评价体系中并不重视此类成果(缺乏创新)。我们的研究中有做少量的相关工作，但考虑篇幅，将作为一个独立项目进行介绍。

 
